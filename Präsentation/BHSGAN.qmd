---
title: "Bridging the Gap Between $f$-GANs and Bayes Hilbert Spaces"
subtitle: " Linus Lach*, Alexander Fottner, Yarema Okhrin"
format:
  revealjs:
    theme: [solarized, custom.scss]
    toc: true
    toc-title: "Content"
    toc-depth: 1
    progress: true
    slide-number: true
    fig-cap-location: top
    include-in-header: 
      text: |
        <style>
        .center-xy {
          margin: 0;
          position: absolute;
          margin-top: 50vh;
        }
        </style>
    logo: pictures/Logos.svg
    bibliography: references.bib
---


# $f$-GANs



::: {style="display:none;"}
(

\def\bE{{\mathbb{E}}}

\def\bR{{\mathbb{R}}}

\def\cB{{\mathcal{B}}}

\def\rmd{{\mathrm{d}}}

\def\cT{{\mathcal{T}}}

\def\rmd{{\mathrm{d}}}

)
:::


## $f$-Divergences [@csiszar2004information]

:::{style="font-size:24pt;"}

Quantify dissimilarity between two probability measures $\mu$ and $\nu$ that are absolutely continuous w.r.t to some $\sigma$-finite base measure $\lambda$.^[ $\mu$ is absolutely continuous w.r.t. $\lambda$ if every $\mu$ nullset is a $\lambda$ nullset and vice versa]

$$
  D_{f}(\mu,\nu) = \int_{\mathbb{R}}p_\nu(x)f\left(\frac{p_\mu(x)}{p_\nu(x)}\right)\lambda(\mathrm{d}x).
$$

-   $f:{\mathbb{R}}_+\to{\mathbb{R}}$ convex,

-   lower-semicontinuous, and

-   satisfying $f(1) = 0$.

:::

:::{style="text-align:center;"}

$f$ is called **divergence generating function**

:::

## Examples of $f$-divergences generating functions

![](pictures/f_divergences.svg)

## Estimating $f$-divergences is hard!

-   Nonparametric estimation 

-   Only finite samples available

-   Highdimensional setting

**Solution:**
Find easy to estimate lower bound [@nguyen2010estimating]:

\begin{equation}
  D_f(\mu,\nu) \geq \sup_{T\in\cT}\bigg\{\bE_\mu(T)- \bE_\nu(f^*\circ T)\bigg\}
\end{equation}

## Components of the lower bound

\begin{equation}
  D_f(\mu,\nu) \geq \sup_{T\in\cT}\bigg\{\bE_\mu(T)- \bE_\nu(f^*\circ T)\bigg\}
\end{equation}

- $\cT$ arbitrary class of measurable functions $T:\Omega \to \mathrm{dom}(f^*)$

- Fenchel conjugate $f^*$ of $f$: $f^*(y):= \sup_{x\in\mathrm{dom}(f)} \{xy - f(x)\}$

## Fenchel conjugates

::::{style="font-size:21pt;"}

:::{.callout-note title="Theorem (@textbook)" icon="false"}

:::{style="font-size:21pt;"}

Let $f:\bR^n\to\bR$ be a convex function. If $f$ is lower semi-continuous, then the duality $f^{**}(x) = f(x)$ for all $x\in \bR^n$ holds.
    
:::

:::

This theorem also works for concave functions:

:::{.callout-note title="Theorem (@textbook)" icon="false"}

:::{style="font-size:21pt;"}

Let $f:\bR^n\to\bR$ be a concave function. If $f$ is upper semi-continuous, then the duality $f_{**}(x) = f(x)$ for all $x\in \bR^n$ holds.

:::
 
:::

Here, 
\begin{equation}
f_*(y) := \inf_{x\in\mathrm{dom}(g)}\{xy-f(x)\}.
\end{equation}

::::

## Optimizing the lower bound 

If $f$ is not only convex and lower semicontinuous but $f \in \mathcal{C}^1$, then the bound
\begin{equation}
  D_f(\mu,\nu) \geq \sup_{T\in\cT}\bigg\{\bE_\mu(T)- \bE_\nu(f^*\circ T)\bigg\}
\end{equation}

is tight and the supremum is attained at

\begin{equation}
  \tilde{T} (x) = f^\prime\left(\frac{p_\mu(x)}{p_\nu(x)}\right).
\end{equation}



## Generative Adversarial Networks (GANs) in a nutshell

- Sample from an unknown distribution

- Alternately:
  - Train a generative model that creates samples from an unknown distribution
  
  - Train a discriminatory model (binary classifier) that incentives the generator to produce more realistic examples

## Training objective for $f$-GANs

@nowozin2016f extended the work of @goodfellow2014generative to a generalized optimization problem: 

\begin{equation}

\min_{\theta} \max_{\omega} \bigg[ \mathbb{E}_{\mu}(T_\omega) - \mathbb{E}_{\nu_{\theta}} (f^*\circ T_\omega) \bigg]

\end{equation}

$\theta$ (generator) and $\omega$ (discriminator) are each parameters of neural networks.


# Bayes Hilbert Spaces

## General idea

1.    Construct a linear space for proportional $\sigma$-finite measures, including probability measures.

2.    Consider the subspace of square-log-integrable densities.^[In this setting, measures can be identified with their corresponding Radon-Nikodym density.]

3.    Define an inner product on this subspace using the centered log ratio for measures.

4.    Obtain a Hilbert space structure that allows for a straight forward interpretation of distances between densities.

## Some definitions


-   Define $\mathcal{M}(\lambda)$ as the set of measures on $(\Omega, \mathcal{B})$ that are equivalent to the base measure $\lambda$. 

-   Two measures $\mu,\nu \in \mathcal{M}(\lambda)$ are defined as $B$-equivalent denoted by $\mu =_{B(\lambda)} \nu$ if there exists a constant $c>0$ such that $\mu(A)=c\nu(A)$ for all $A\in \mathcal{B}$. Then, $=_{B(\lambda)}$ is an equivalence relation on $\mathcal{M}(\lambda)$

- Finally, define $B(\lambda) :=\mathcal{M}(\lambda)/=_{B(\lambda)}$ 

## Bayes Linear Space

::::{style="font-size:24pt;"}

For two measures $\mu,\nu\in B(\lambda)$ define the perturbation of $\mu$ by $\nu$ over some set $R\in\mathcal{B}$ as

\begin{equation}
  (\mu \oplus \nu)(R) := \int_R\frac{\rmd\mu}{\rmd\lambda}\frac{\rmd \nu}{\rmd \lambda}\: \rmd \lambda
\end{equation}

and the powering of $\mu$ by $\alpha\in\mathbb{R}$ as

\begin{equation}
  (\alpha \odot \mu)(R) := \int_R\left(\frac{\rmd \mu}{\rmd \lambda}\right)^\alpha \rmd \lambda
\end{equation}

:::{.callout-note title="Theorem (@boogaart2010bayes)" icon="false"}

:::{style="font-size:24pt;"}

$(B(\lambda),\oplus,\odot)$ is a linear space.

:::
:::

::::


## The centered log ratio

For $p\geq 1$ define

$$
  B^p(\lambda) = \left\{\mu\in B(\lambda): \int_{\mathbb{R}}\left|\log\left(\frac{{\mathrm{d}}\mu}{{\mathrm{d}}\lambda}\right) \right|^p{\mathrm{d}}\lambda <+\infty\right\}.
$$

The $\mathrm{clr}$ of $\mu\in B^p(\lambda)$ is defined as

$$
\mathrm{clr}(\mu) = \log\left(\frac{{\mathrm{d}}\mu}{{\mathrm{d}}\lambda}\right) - \int \log\left(\frac{{\mathrm{d}}\mu}{{\mathrm{d}}\lambda}\right)\: {\mathrm{d}}\lambda.
$$


## Bayes Hilbert spaces

:::{.callout-note title="Theorem (@van2014bayes)" icon="false"}

$B^2(\lambda)$ equipped with the inner product 

:::{style="font-size:20pt;"}

\begin{equation}
   \int \left(\log(f(x))-\int\log(f(y))\lambda(\rmd y)\right)
      \left(\log(g(x))-\int\log(g(y))\lambda(\rmd y)\right)
      \lambda(\rmd y)
\end{equation}

:::

denoted by $\langle f,g\rangle_{B^2(\lambda)}$ with $f,g$ densities in $B^2(\lambda)$ is a separable Hilbert space.

:::

:::{.callout-note title="Theorem (@van2014bayes)" icon="false"}

:::{style="font-size:20pt;"}

$\mathrm{clr}:B^2(\lambda)\to L_0^2(\lambda)$ is an isometry of Hilbert spaces.

:::

:::


# Mixed conjugates and pseudo $f$-divergences

## Mixed conjugates

:::{style="font-size:24pt;"}

Consider $f:[0,\infty)\to\bR$ continuous, satisfying $f(1)=0$, concave on some interval $[0,a),\, a>0$, and convex on $[a,\infty)$.
The mixed conjugate $f^*_*$ of $f$ is defined by

\begin{equation}\label{def:mixed_conjugate}
    f^*_*(t) := \sup_{x\in [a,\infty)}\{tx-f(x)\}\mathbb{I}_{\{t\in M\}}+\inf_{x\in [0,a)}\{tx-f(x)\}\mathbb{I}_{\{t\in N\}}
\end{equation}

where

:::

:::{style="font-size:22pt;"}

\begin{align*}
    M &:= \left\{t\in\mathrm{dom}(f^*_*):\underset{x\in\mathrm{dom(f)}}{\mathrm{argmax}}(tx-f(x))\in [a,\infty) \right\},\\
    N &:= \left\{t\in\mathrm{dom}(f^*_*):\underset{x\in\mathrm{dom(f)}}{\mathrm{argmin}}(tx-f(x))\in  [0,a) \right\}.
\end{align*}

:::

## Mixed conjugates and pseudo $f$-divergences

:::{.callout-note title="Lemma" icon="false"}

For any continuous function $f:[0,\infty)\to\bR$ that is convex on $[a,\infty),\:a> 0$, concave on $[0,a)$, and satisfies $\lim_{x\to\infty} f(x) = +\infty$, the mixed conjugate satisfies:

1.    $M\cap N = \emptyset$ and $M\cup N = \mathrm{dom}(f^*_*)$

2.    $f^{**}_{**} = f$ for almost all $t\in\mathrm{dom}(f)$.
    
:::

:::{.callout-note title="Lemma" icon="false"}

Let $f:[0,\infty)\to [0,\infty)$ be concave on some interval $[0,a),\: a>0$ and convex on $[a,\infty)$.
For any probability measures $\mu,\nu\in B^2(\lambda),\: D_{f}(\mu,\nu)$ is well-defined in the sense that $D_{f}(\mu,\nu)\geq 0$ and $D_{f}(\mu,\nu) = 0 \iff \mu = \nu\quad \lambda-\text{a.s.}$

:::

## Pseudo divergence generating functions

![](pictures/pseudof.svg)

## Lower bounds for pseudo $f$-divergences

:::{.callout-note title="Corollary" icon="false"}

 For any function $f$ satisfying the conditions of the previous Lemma, the lower bound  
 \begin{equation}\label{eq:lower_bound}
     D_f(\mu,\nu) \geq \sup_{\bar T\in C(M^*)\cup C(N^*)}\bigg\{\bE_\mu(\bar T)- \bE_\nu(f^*_*\circ \bar T)\bigg\}
 \end{equation}
 
 holds.

:::


## Optimizing lower bounds for pseudo $f$-divergences

:::{.callout-note title="Theorem" icon="false"}

Let $f:[0,\infty) \to\bR$, convex on $[a,\infty)$ with $0< a<\infty$, concave on $[0,a)$, assume that $f$ is twice continuously differentiable, and that $((f^*_*)^{\prime})^{-1}$ is well-defined.
Furthermore, let $\mu,\nu\in B^2(\lambda)$ with $p_\mu(x):= \frac{\rmd \mu}{\rmd \lambda}(x) ,\, p_\nu(x):=\frac{\rmd \nu}{\rmd \lambda}(x)$ .
Then, $\tilde T(x) := f^\prime \left(\frac{p_\mu(x)}{p_\nu(x)}\right)$ is an optimizer for
\begin{equation}
    \sup_{T\in C(M^*)}\bigg\{\left(\bE_\mu(T)- \bE_\nu(f^*_*\circ T)\right)\bigg\} +
    \inf_{T\in C(N^*)}\bigg\{(\bE_\mu(T)- \bE_\nu(f^*_*\circ T))\bigg\}.\label{eq:supE_infE}
   % \sup_{T\in\cT}\left\{\bE_\mu(T) - \bE_\nu(f^*_*\circ T) \right\}.\label{eq:supE}
\end{equation}
Here, $C(M^*)$ ($C(N^*)$) denotes the set of continuous functions $T:\Omega\to \mathrm{dom}(f^*_*)$ such that $f^*_* \circ T$ is convex (concave).

:::

# Bayes Hilbert space GANs

## Bayes Hilbert space divergence

Consider the function 

\begin{equation}\label{eq:f_BHS}
    f_{BHS}:[0,\infty)\to\bR,\quad x\mapsto x\log(x)^2
\end{equation}

that is convcave on $[0,\exp(-1)]$ and convex for $x>\exp(-1)$.

Applying the previous lemma implies that $f_{BHS}$ induces a pseudo $f$-divergence called $f_{BHS}$-divergence.

## Connection to Bayes Hilbert spaces

\begin{align}
  D_{f_{BHS}}(\mu,\nu) &= d_{B^2(\mu)}(\mu,\nu) + \mathbb{E}_{\mu}(\log(\mu \ominus \nu))^2\\
\end{align}

:::{.callout-note title="Corollary" icon="false"}

For $\tilde T(x) = f_{BHS}^\prime\left(\frac{p_\mu(x)}{p_\nu(x)}\right)$ we have 
\begin{align}
      \sup_{T\in C}&\left\{(\bE_\mu(T) - \bE_\nu(f^*_*\circ T)) \mathbb{I}_{\{T\in M_C\}}\right\}+ \\
    &\qquad\inf_{T\in C}\left\{(\bE_\mu(T) - \bE_\nu(f^*_*\circ T)) \mathbb{I}_{\{T\in N_C\}}\right\}\\
    &=\bE_\mu(\tilde T)- \bE_\nu(f^*_*\circ \tilde T)\\
    & = D_{f_{\mathrm{BHS}}}(\mu,\nu).
\end{align}
:::

## Bayes Hilbert space GAN

Optimization problem:

\begin{equation}

\min_{\vartheta}\max_{\omega}F(\vartheta,\omega) := \min_{\vartheta}\max_{\omega} \bE_\mu(\bar{T}_\omega)-\bE_{\nu_\vartheta}(f^*_*\circ \bar{T}_\omega).

\end{equation}

where

\begin{equation}
  \bar{T}_\omega(x) = g_f\circ V_\omega(x).
\end{equation}

$V_\omega$ denotes the discriminatory model with no restrictions on the output, i.e., $\mathrm{Im}(V_\omega) \subseteq \bR$ and $g_f$ a final output activation function depending on the domain of $f^*_*$.


## Computational results 

::::{.columns}

:::{.column width="50%"}

:::{style="font-size:24pt;"}

| **Model**         | **FID**          |
|-------------------|------------------|
| BHSGAN            | 31.26 ± 0.08     |
| KL GAN            | 37.50 ± 0.13     |
| Reverse KL GAN    | 85.27 ± 0.19     |
| Pearson GAN       | 33.60 ± 0.38     |
| GAN               | 33.60 ± 0.10     |
| WGAN              | 30.81 ± 0.12     |

:::
:::
:::{.column width="50%"}

![](pictures/CIFAR_Summary.png)
:::

::::




# References